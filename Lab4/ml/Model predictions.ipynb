{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\icarter\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\icarter\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azure.cosmosdb.table.tableservice import TableService\n",
    "import creds\n",
    "import datetime\n",
    "\n",
    "# Load model\n",
    "model_path = ('model_predlabel_6hwill_rain.pkl')\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Build data for prediction\n",
    "table_service = TableService(account_name=creds.ACCNAME, account_key=creds.KEY)\n",
    "\n",
    "def get_df(table):\n",
    "    now = datetime.datetime.now() - datetime.timedelta(days = 2, hours = 1)\n",
    "    datefilter = \"RowKey ge '\" + str(now.timestamp()) + \"'\"\n",
    "    weather_gen = table_service.query_entities(table, filter=datefilter)\n",
    "    to_df = []\n",
    "    for weather in weather_gen:\n",
    "        to_df.append(weather)\n",
    "    df = pd.DataFrame(to_df)\n",
    "    return df\n",
    "\n",
    "def get_and_clean_cur_weather():\n",
    "    table = 'WeatherAPICurrent'\n",
    "    df = get_df(table).drop(['PartitionKey','RowKey','Timestamp','etag','last_updated','wind_degree'], axis=1)\n",
    "    cols = df.columns.drop(['condition','localtime','wind_dir'])\n",
    "    df[cols] = df[cols].apply(pd.to_numeric)\n",
    "    df = df.sort_values('localtime_epoch').reset_index(drop=True)\n",
    "    df['pressure_mb'] = df['pressure_mb'].astype(int)\n",
    "    df['uv'] = df['uv'].astype(int)\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[8:10] + cols[6:8] + cols[:6] + cols[10:]\n",
    "    df = df[cols]\n",
    "    df['localtime'] = pd.to_datetime(df['localtime_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "#     df['last_updated'] = pd.to_datetime(df['last_updated_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "    df = df.drop(['localtime_epoch','last_updated_epoch'], axis=1)\n",
    "#     df['year'] = df['localtime'].dt.year\n",
    "#     df['month'] = df['localtime'].dt.month\n",
    "#     df['day'] = df['localtime'].dt.day\n",
    "#     df['hour'] = df['localtime'].dt.hour\n",
    "#     df['minute'] = df['localtime'].dt.minute\n",
    "#     df['minute'] = df['minute'].map(floor)\n",
    "    return df\n",
    "\n",
    "def get_and_clean_for_weather():\n",
    "    table = 'WeatherAPIForecast'\n",
    "    df = get_df(table).drop(['PartitionKey','RowKey','Timestamp','etag'], axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    cols = df.columns.drop(['localtime','D0_condition','D1_condition','D2_condition','D0_date','D1_date','D2_date'])\n",
    "    df[cols] = df[cols].apply(pd.to_numeric)\n",
    "    for col in cols:\n",
    "        try:\n",
    "            df[col] = df[col].astype(int)\n",
    "        except:\n",
    "            continue\n",
    "    df = df.sort_values('localtime_epoch')\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-2:] + cols[:-2]\n",
    "    df = df[cols]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['localtime'] = pd.to_datetime(df['localtime_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "    df['D0_date'] = pd.to_datetime(df['D0_date_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "    df['D1_date'] = pd.to_datetime(df['D1_date_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "    df['D2_date'] = pd.to_datetime(df['D2_date_epoch'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Asia/Phnom_Penh')\n",
    "    df = df.drop(['D0_date_epoch','D1_date_epoch','D2_date_epoch'], axis=1)\n",
    "    \n",
    "    keys = {}\n",
    "    for day in df['D0_date'].unique():\n",
    "        keys[day] = {}\n",
    "        for D in ['D0_', 'D1_', 'D2_']:\n",
    "            for column in [x for x in df.columns if D in x]:\n",
    "                try:\n",
    "                    if df[column].dtype in ['float64','int64','float32','int32']:\n",
    "                        keys[day]['cur' + column + '_mean'] = df[df['D0_date'] == day][column].describe()[1]\n",
    "                        keys[day]['cur' + column + '_std'] = df[df['D0_date'] == day][column].describe()[2]\n",
    "                        keys[day]['cur' + column + '_states'] = len(df[df['D0_date'] == day][column].unique())\n",
    "                    elif df[column].dtype in ['O', 'string']: # Sometimes this line doesnt work and gives a TypeError: data type 'string' not understood; not clear why it throws this error, hence the try/except\n",
    "                        keys[day]['cur' + column + '_consensus'] = (df[df['D0_date'] == day][column].value_counts()[0] / df[df['D0_date'] == day][column].shape[0])\n",
    "                        keys[day]['cur' + column + '_states'] = len(df[df['D0_date'] == day][column].unique())\n",
    "                except:\n",
    "                    if df[column].dtype in ['float64','int64','float32','int32']:\n",
    "                        keys[day]['cur' + column + '_mean'] = df[df['D0_date'] == day][column].describe()[1]\n",
    "                        keys[day]['cur' + column + '_std'] = df[df['D0_date'] == day][column].describe()[2]\n",
    "                        keys[day]['cur' + column + '_states'] = len(df[df['D0_date'] == day][column].unique())\n",
    "                    elif df[column].dtype in ['O']:\n",
    "                        keys[day]['cur' + column + '_consensus'] = (df[df['D0_date'] == day][column].value_counts()[0] / df[df['D0_date'] == day][column].shape[0])\n",
    "                        keys[day]['cur' + column + '_states'] = len(df[df['D0_date'] == day][column].unique())\n",
    "    # get the days lined up in a dict\n",
    "\n",
    "#     today_cols = [x for x in keys[df['D0_date'].unique()[0]].keys() if 'D0' in x]\n",
    "    yesterday_cols = [x for x in keys[df['D0_date'].unique()[0]].keys() if 'D1' in x]\n",
    "    daybefore_cols = [x for x in keys[df['D0_date'].unique()[0]].keys() if 'D2' in x]\n",
    "\n",
    "    aligned_for1 = pd.DataFrame(columns=['D0_date'])\n",
    "    aligned_for2 = pd.DataFrame(columns=yesterday_cols)\n",
    "    aligned_for3 = pd.DataFrame(columns=daybefore_cols)\n",
    "    aligned_for = pd.concat([aligned_for1, aligned_for2, aligned_for3])\n",
    "\n",
    "    forecast_avg = pd.DataFrame.from_dict(keys, orient='index')\n",
    "\n",
    "    for day in df['D0_date'].unique()[2:]:\n",
    "        target_day = day\n",
    "#         today = target_day\n",
    "        yesterday = target_day - pd.DateOffset(1)\n",
    "        daybefore = target_day - pd.DateOffset(2)\n",
    "\n",
    "        temp = pd.Series({'D0_date': day})\n",
    "        temp = temp.append(forecast_avg.loc[yesterday][yesterday_cols])\n",
    "        temp = temp.append(forecast_avg.loc[daybefore][daybefore_cols])\n",
    "\n",
    "        aligned_for = aligned_for.append(temp, ignore_index=True)\n",
    "        \n",
    "    del temp\n",
    "    \n",
    "    aligned_for['D0_date'] = pd.to_datetime(aligned_for['D0_date'])\n",
    "        \n",
    "    df = pd.merge(df, aligned_for, on='D0_date')\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_all_clean_weather():\n",
    "    left = get_and_clean_cur_weather()\n",
    "#     print('%s rows in cur' % left.shape[0])\n",
    "    right = get_and_clean_for_weather()\n",
    "#     print('%s rows in for' % right.shape[0])\n",
    "    df = pd.merge_asof(left, right, direction='nearest') # This will get the nearest forecast to the current weather, whether taking place before or after, without any time constraint\n",
    "#     df = pd.merge_asof(left, right, direction='nearest', tolerance=pd.Timedelta('120s')) # This will get the nearest forecast to the current weather, whether taking place before or after, and within 2 minutes\n",
    "#     df = pd.merge_asof(left, right, direction='backward') # This will get the latest forecast up to the current time, but not after, regardless of how long away\n",
    "\n",
    "    # Running 'backward' creates a gaps of 20 minutes or longer between cur and for associations, as the forecast seems to take place after the current weather is posted to the API. therefore prefer to use nearest for matching, as we can run the model a few moments after the forecast comes in to make the prediction.\n",
    "    \n",
    "#     print('%s rows before drop' % df.shape[0])\n",
    "#     df.dropna(inplace=True)\n",
    "#     print('%s rows after drop' % df.shape[0])\n",
    "    return df\n",
    "\n",
    "def get_conditions_set():\n",
    "    weather_gen = table_service.query_entities('WeatherAPICurrent', select='condition')\n",
    "    to_df = []\n",
    "    for weather in weather_gen:\n",
    "        to_df.append(weather)\n",
    "    curdf = pd.DataFrame(to_df).drop('etag', axis=1)\n",
    "    weather_gen = table_service.query_entities('WeatherAPIForecast', select=\"D0_condition, D1_condition, D2_condition\")\n",
    "    to_df = []\n",
    "    for weather in weather_gen:\n",
    "        to_df.append(weather)\n",
    "    fordf = pd.DataFrame(to_df).drop('etag', axis=1)\n",
    "    conditions_set = set()\n",
    "    for x in curdf.condition.unique():\n",
    "        conditions_set.add(x)\n",
    "    for col in ['D0_condition', 'D1_condition', 'D2_condition']:\n",
    "        for x in fordf[col].unique():\n",
    "            conditions_set.add(x)\n",
    "    return conditions_set\n",
    "\n",
    "def get_wind_dir_set():\n",
    "    weather_gen = table_service.query_entities('WeatherAPICurrent', select='wind_dir')\n",
    "    to_df = []\n",
    "    for weather in weather_gen:\n",
    "        to_df.append(weather)\n",
    "    curdf = pd.DataFrame(to_df).drop('etag', axis=1)\n",
    "    conditions_set = set()\n",
    "    for x in curdf.wind_dir.unique():\n",
    "        conditions_set.add(x)\n",
    "    return conditions_set\n",
    "\n",
    "def cleaned_data():\n",
    "    df = get_all_clean_weather().dropna(subset=['localtime_epoch']).reset_index()\n",
    "    \n",
    "    df.loc[df['precip_mm'] > 0, 'is_raining'] = 1\n",
    "    df['is_raining'] = df['is_raining'].fillna(0)\n",
    "    \n",
    "    drop_columns = ['index','localtime','localtime_epoch','D0_date','D1_date','D2_date']\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "    \n",
    "    for col in ['condition','D0_condition', 'D1_condition', 'D2_condition']:\n",
    "        for condition in get_conditions_set():\n",
    "            new_condition = col + '_' + condition.replace(' ', '_') # To satisfy the Table Service requirement that column names do not have spaces\n",
    "            df.loc[df[col] == condition, new_condition] = int(1)\n",
    "            df[new_condition] = df[new_condition].fillna(0)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    for wind in get_wind_dir_set():\n",
    "        new_wind_dir = 'wind_dir_' + wind\n",
    "        df.loc[df['wind_dir'] == wind, new_wind_dir] = int(1)\n",
    "        df[new_wind_dir] = df[new_wind_dir].fillna(0)\n",
    "    df.drop('wind_dir', axis=1, inplace=True)\n",
    "        \n",
    "    print(df.shape)\n",
    "    df.dropna(inplace=True)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 208)\n",
      "(98, 208)\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "data = cleaned_data().iloc[-1]\n",
    "\n",
    "# Get the input data as a numpy array\n",
    "npdata = np.array(data).reshape(1, -1)\n",
    "# Get a prediction from the model\n",
    "predictions = model.predict(npdata)\n",
    "# Return the predictions as any JSON serializable format\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['pred6hrain'] = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W/\"datetime\\'2020-07-17T01%3A51%3A11.89686Z\\'\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = dict(data)\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = str(data_dict[key])\n",
    "data_dict['PartitionKey'] = 'Predictions'\n",
    "data_dict['RowKey'] = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')\n",
    "table_service.insert_entity('Predictions', data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(data):\n",
    "    data['prediction_date'] = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')\n",
    "    dump = []\n",
    "    dump.append('CREATE TABLE IF NOT EXISTS predictions (')\n",
    "    for column in data.index:\n",
    "        dump.append(' \"{0}\" REAL,'.format(column))\n",
    "    dump.append(' PRIMARY KEY (\"prediction_date\")')\n",
    "    dump.append(')')\n",
    "    with open('predictions.txt', 'w') as f:\n",
    "        for line in dump:\n",
    "            f.write(line + '\\n')\n",
    "        f.write(\"\"\"CLUSTERED INTO 4 SHARDS\n",
    "WITH (\n",
    "    \"allocation.max_retries\" = 5,\n",
    "    \"blocks.metadata\" = false,\n",
    "    \"blocks.read\" = false,\n",
    "    \"blocks.read_only\" = false,\n",
    "    \"blocks.read_only_allow_delete\" = false,\n",
    "    \"blocks.write\" = false,\n",
    "    column_policy = 'strict',\n",
    "    \"mapping.total_fields.limit\" = 1000,\n",
    "    max_ngram_diff = 1,\n",
    "    max_shingle_diff = 3,\n",
    "    number_of_replicas = '0-1',\n",
    "    refresh_interval = 1000,\n",
    "    \"routing.allocation.enable\" = 'all',\n",
    "    \"routing.allocation.total_shards_per_node\" = -1,\n",
    "    \"translog.durability\" = 'REQUEST',\n",
    "    \"translog.flush_threshold_size\" = 536870912,\n",
    "    \"translog.sync_interval\" = 5000,\n",
    "    \"unassigned.node_left.delayed_timeout\" = 60000,\n",
    "    \"warmer.enabled\" = true,\n",
    "    \"write.wait_for_active_shards\" = 'ALL'\n",
    ")\n",
    "\"\"\")\n",
    "    print('Remember to change integers to INT in the text file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datefilter = \"RowKey ge '\" + datetime.datetime.utcnow().date().strftime('%Y-%m-%d') + \"'\"\n",
    "weather_gen = table_service.query_entities('Predictions', filter=datefilter)\n",
    "to_df = []\n",
    "for weather in weather_gen:\n",
    "    to_df.append(weather)\n",
    "df = pd.DataFrame(to_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability component\n",
    "\n",
    "features = model.feature_importances_\n",
    "temp = data.copy().reset_index()\n",
    "temp['feature_importances'] = features\n",
    "temp = temp.sort_values(by='feature_importances', ascending=False)\n",
    "temp = temp.reset_index(drop=True)\n",
    "temp.rename(columns={temp.columns[0]:'feature',temp.columns[1]:'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.utcnow().date().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
